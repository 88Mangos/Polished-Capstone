{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Package Imports ###\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import functools\n",
    "import mitdeeplearning as mdl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Package Setup ###\n",
    "start_time = time.localtime(time.time())\n",
    "\n",
    "CWD = os.getcwd()\n",
    "# print(CWD)\n",
    "\n",
    "matplotlib.rcParams['font.family'] = \"Times New Roman\"\n",
    "\n",
    "### Ensure training on GPU ###\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions: Load and Visualize Dataset ###\n",
    "\n",
    "### Function: load dataset ###\n",
    "@keras.saving.register_keras_serializable(package='capstone',name='load_dataset')\n",
    "def load_dataset(path_to_training_data):\n",
    "    with h5py.File(path_to_training_data) as f:\n",
    "        # Print the keys (names) of all groups and datasets in the file\n",
    "        print(\"Keys:\", list(f.keys()))\n",
    "\n",
    "        # Iterate through each key and print more detailed information\n",
    "        for key in f.keys():\n",
    "            if isinstance(f[key], h5py.Dataset):\n",
    "                print(f\"Dataset: {key}\")\n",
    "                print(\"  Shape:\", f[key].shape)\n",
    "                print(\"  Data type:\", f[key].dtype)\n",
    "                \n",
    "    ### Instantiate Loader Function ###\n",
    "    return mdl.lab2.TrainingDatasetLoader(path_to_training_data)\n",
    "\n",
    "### Function: visualize dataset ###\n",
    "@keras.saving.register_keras_serializable(package='capstone',name='visualize_dataset')\n",
    "def visualize_dataset(loader):\n",
    "    ### Visualize our data ###\n",
    "    number_of_training_examples = loader.get_train_size()\n",
    "    print(number_of_training_examples)\n",
    "    (images, labels) = loader.get_batch(100)\n",
    "    malignant_images = images[np.where(labels==1)[0]]\n",
    "    benign_images = images[np.where(labels==0)[0]]\n",
    "\n",
    "    idx_malignant = 23\n",
    "    idx_benign = 9\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(malignant_images[idx_malignant])\n",
    "    plt.title(\"Malignant\"); plt.grid(False)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(benign_images[idx_benign])\n",
    "    plt.title(\"Benign\"); plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instantiate Loaders: train/val 90/10 ###\n",
    "loader_ISIC = load_dataset(f'{CWD}/datasets/split-90train-10val/train_ISIC.h5')\n",
    "loader_ISIC_DiDI = load_dataset(f'{CWD}/datasets/split-90train-10val/train_ISIC_DiDI.h5')\n",
    "loader_ISIC_ArGI = load_dataset(f'{CWD}/datasets/split-90train-10val/train_ISIC_ArGI.h5')\n",
    "\n",
    "loader_val_ISIC = load_dataset(f'{CWD}/datasets/split-90train-10val/val_ISIC.h5')\n",
    "loader_val_ISIC_DiDI = load_dataset(f'{CWD}/datasets/split-90train-10val/val_ISIC_DiDI.h5')\n",
    "loader_val_ISIC_ArGI =  load_dataset(f'{CWD}/datasets/split-90train-10val/val_ISIC_ArGI.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Visualize Training Datasets ###\n",
    "# visualize_dataset(loader_ISIC)\n",
    "# visualize_dataset(loader_ISIC_DiDI)\n",
    "# visualize_dataset(loader_ISIC_ArGI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard CNN ###\n",
    "\n",
    "\"\"\"\n",
    "Instantiation helper functions:\n",
    "- resize_images: used in lambda function layer to resize the input images\n",
    "\"\"\"\n",
    "@keras.saving.register_keras_serializable(package='capstone', name='resize_images')\n",
    "def resize_images(x):\n",
    "    return tf.image.resize(x, (64, 64))\n",
    "\n",
    "\"\"\"\n",
    "Instantiation function:\n",
    "- make_standard_ResNet50_V2: instantiates a new keras model with the ResNet50v2 CNN architecture. \n",
    "\"\"\"\n",
    "@keras.saving.register_keras_serializable(package='capstone', name='make_standard_ResNet50_V2')\n",
    "def make_standard_ResNet50_V2(n_outputs = 1):\n",
    "    \n",
    "    Resize = tf.keras.layers.Lambda(resize_images)\n",
    "    Flatten = tf.keras.layers.Flatten\n",
    "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
    "    ResNet50V2 = tf.keras.applications.ResNet50V2(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\", # Utilizing Transfer Learning, also maintains consistency\n",
    "        input_tensor=None,\n",
    "        input_shape=(64,64,3),\n",
    "        pooling=None,\n",
    "        classes=1000,\n",
    "        classifier_activation=\"softmax\",\n",
    "    )\n",
    "    ResNet50V2 = tf.keras.Model(inputs = ResNet50V2.layers[1].input, \n",
    "                                outputs = ResNet50V2.layers[-1].output)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(Resize)\n",
    "    model.add(ResNet50V2)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dense(n_outputs, activation=None))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DB-VAE ###\n",
    "\n",
    "\"\"\"\n",
    "Instantiation helper functions:\n",
    "- make_decoder_network: creates decoder section of the debiasing-variational autoencoder. \n",
    "    Structure is akin to ResNet50v2 except inverted.\n",
    "- sampling: VAE reparameterization trick (Amini 2024)\n",
    "- vae_loss_function: as its name suggests, a VAE loss function, also a helper function for debiasing_loss_function\n",
    "- debiasing_loss_function: as its name suggests, a loss function for the debiasing portion of the DB-VAE\n",
    "\n",
    "\"\"\"\n",
    "@keras.saving.register_keras_serializable(package='capstone', name='make_decoder_network')\n",
    "def make_decoder_network(latent_dim = 100, n_filters = 12 ):\n",
    "    \"\"\"\n",
    "    Layer Types, Functional Definition\n",
    "    \"\"\"\n",
    "    Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu')\n",
    "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
    "    Reshape = tf.keras.layers.Reshape \n",
    "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "    LeakyReLU = tf.keras.layers.LeakyReLU\n",
    "    # Decoder\n",
    "    decoder = tf.keras.Sequential([\n",
    "        Dense(units=4*4*6*n_filters),\n",
    "        Reshape(target_shape=(4,4,6*n_filters)),\n",
    "\n",
    "        Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "\n",
    "        Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "\n",
    "        Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "\n",
    "        Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', activation='sigmoid')\n",
    "    ])\n",
    "    return decoder\n",
    "\n",
    "@keras.saving.register_keras_serializable(package='capstone', name='sampling_VAE_reparameterization')\n",
    "def sampling(z_mean, z_logsigma):\n",
    "    batch, latent_dim = z_mean.shape\n",
    "    epsilon = tf.random.normal(shape=(batch, latent_dim))\n",
    "    z = z_mean + tf.math.exp(0.5 * z_logsigma) * epsilon\n",
    "    return z\n",
    "\n",
    "@keras.saving.register_keras_serializable(package='capstone', name='vae_loss_function')\n",
    "def vae_loss_function(x, x_recon, mu, logsigma, kl_weight=0.0005):\n",
    "  latent_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1.0 - logsigma, axis=1)\n",
    "  reconstruction_loss = tf.reduce_mean(tf.abs(x-x_recon), axis=(1,2,3))\n",
    "  vae_loss = kl_weight * latent_loss + reconstruction_loss\n",
    "  return vae_loss\n",
    "\n",
    "@keras.saving.register_keras_serializable(package='capstone',name='debiasing_loss_function')\n",
    "def debiasing_loss_function(x, x_pred, y, y_logit, mu, logsigma):\n",
    "  vae_loss = vae_loss_function(x, x_pred, mu, logsigma)\n",
    "  classification_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_logit)\n",
    "  malignance_indicator = tf.cast(tf.equal(y, 1), tf.float32)\n",
    "  total_loss = tf.reduce_mean(\n",
    "      classification_loss +\n",
    "      malignance_indicator * vae_loss\n",
    "  )\n",
    "  return total_loss, classification_loss\n",
    "\n",
    "\"\"\"\n",
    "Instantiation:\n",
    "Class definition of the DB_VAE\n",
    "\"\"\"\n",
    "@keras.saving.register_keras_serializable(package='capstone')\n",
    "class DB_VAE(tf.keras.Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(DB_VAE, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    # Define the number of outputs for the encoder. Recall that we have\n",
    "    # `latent_dim` latent variables, as well as a supervised output for the\n",
    "    # classification.\n",
    "    num_encoder_dims = 2*self.latent_dim + 1\n",
    "\n",
    "    self.encoder = make_standard_ResNet50_V2(num_encoder_dims)\n",
    "    self.decoder = make_decoder_network()\n",
    "\n",
    "  def encode(self, x):\n",
    "    encoder_output = self.encoder(x)\n",
    "    y_logit = tf.expand_dims(encoder_output[:, 0], -1)\n",
    "    z_mean = encoder_output[:, 1:self.latent_dim+1]\n",
    "    z_logsigma = encoder_output[:, self.latent_dim+1:]\n",
    "\n",
    "    return y_logit, z_mean, z_logsigma\n",
    "\n",
    "  def reparameterize(self, z_mean, z_logsigma):\n",
    "    z = sampling(z_mean, z_logsigma)\n",
    "    return z\n",
    "\n",
    "  def decode(self, z):\n",
    "    reconstruction = self.decoder(z)\n",
    "    return reconstruction\n",
    "\n",
    "  def call(self, x):\n",
    "    y_logit, z_mean, z_logsigma = self.encode(x)\n",
    "    z = self.reparameterize(z_mean, z_logsigma)\n",
    "    recon = self.decode(z)\n",
    "    return y_logit, z_mean, z_logsigma, recon\n",
    "\n",
    "  def predict(self, x):\n",
    "    y_logit, z_mean, z_logsigma = self.encode(x)\n",
    "    return y_logit\n",
    "  \n",
    "\n",
    "\"\"\"\n",
    "Training helper functions:\n",
    "- get_latent_mu: finds mean of latent distribution\n",
    "- get_training_sample_probabilities: recomputes the sampling probabilities for images within a batch \n",
    "    based on how they distribute across the training data\n",
    "\"\"\"\n",
    "# Function to return the means for an input image batch\n",
    "@keras.saving.register_keras_serializable(package='capstone',name='get_latent_mu')\n",
    "def get_latent_mu(images, dbvae, batch_size=1024, latent_dim=100):\n",
    "    N = images.shape[0]\n",
    "    mu = np.zeros((N, latent_dim))\n",
    "    for start_ind in range(0, N, batch_size):\n",
    "        end_ind = min(start_ind+batch_size, N+1)\n",
    "        batch = (images[start_ind:end_ind]).astype(np.float32)/255.\n",
    "        _, batch_mu, _ = dbvae.encode(batch)\n",
    "        mu[start_ind:end_ind] = batch_mu\n",
    "    return mu\n",
    "\n",
    "@keras.saving.register_keras_serializable(package='capstone',name='get_training_sample_probabilities')\n",
    "def get_training_sample_probabilities(images, dbvae, bins=10, smoothing_fac=0.001, latent_dim=100):\n",
    "    print(\"Recomputing the sampling probabilities\")\n",
    "    mu = get_latent_mu(images, dbvae)\n",
    "    training_sample_p = np.zeros(mu.shape[0])\n",
    "    for i in range(latent_dim):\n",
    "        latent_distribution = mu[:,i]\n",
    "        hist_density, bin_edges =  np.histogram(latent_distribution, density=True, bins=bins)\n",
    "        bin_edges[0] = -float('inf')\n",
    "        bin_edges[-1] = float('inf')\n",
    "        bin_idx = np.digitize(latent_distribution, bin_edges)\n",
    "        hist_smoothed_density = hist_density + smoothing_fac\n",
    "        hist_smoothed_density = hist_smoothed_density / np.sum(hist_smoothed_density)\n",
    "        p = 1.0/(hist_smoothed_density[bin_idx-1])\n",
    "        p = p / np.sum(p)\n",
    "        training_sample_p = np.maximum(p, training_sample_p)\n",
    "    training_sample_p /= np.sum(training_sample_p)\n",
    "\n",
    "    return training_sample_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set Training Hyperparameters ###\n",
    "\n",
    "### Hyperparameters for CNN Training ###\n",
    "params_CNN = dict( \n",
    "  batch_size = 32,\n",
    "  num_epochs = 25,\n",
    "  learning_rate = 1e-2, # 5e-4 was initial rate set by Amini\n",
    "  min_lr = 1e-5, # for adaptive learning rate\n",
    "  factor = 0.8, # for adaptive learning rate\n",
    "  patience_lr = 5, # for adaptive learning rate\n",
    "  patience_stop = 10, # for early stopping\n",
    "  optimizer = 'SGD', # 'Adam' or 'SGD'\n",
    ")\n",
    "\n",
    "### Hyperparameters for DB-VAE Training ###\n",
    "params_DB_VAE = dict(\n",
    "    batch_size = 32,\n",
    "    num_epochs = 25, \n",
    "    learning_rate = 5e-4,\n",
    "    latent_dim = 100,\n",
    "    optimizer = 'SGD', # 'Adam' or 'SGD'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package='capstone',name='graph_metrics')\n",
    "def graph_metrics(model_num, completed_epochs, metrics):\n",
    "    size_axis_titles = 16\n",
    "    size_title = 18\n",
    "    size_legend = 14\n",
    "    \n",
    "    fig, [ax_acc, ax_loss] = plt.subplots(1, 2)\n",
    "    fig.set_size_inches((16, 7))\n",
    "    fig.suptitle(f'Model {model_num} Training', fontsize=size_title)\n",
    "    \n",
    "    ax_acc.set_xlabel('Epoch', fontsize=size_axis_titles)\n",
    "    ax_acc.set_ylabel('Accuracy', fontsize=size_axis_titles)\n",
    "    ax_acc.set_xbound(1, completed_epochs+1)\n",
    "    ax_acc.set_ybound(0, 1.0)\n",
    "    ax_acc.plot(metrics['train_acc'], label='Training')\n",
    "    ax_acc.plot(metrics['val_acc'], label='Validation')\n",
    "    ax_acc.legend(loc='upper right', fontsize=size_legend)\n",
    "\n",
    "    ax_loss.set_xlabel('Epoch', fontsize=size_axis_titles)\n",
    "    ax_loss.set_ylabel('Loss', fontsize=size_axis_titles)\n",
    "    ax_loss.set_xbound(1, completed_epochs+1)\n",
    "    ax_loss.set_ybound(0, 2.5)\n",
    "    ax_loss.plot(metrics['train_loss'], label='Training')\n",
    "    ax_loss.plot(metrics['val_loss'], label='Validation')\n",
    "    ax_loss.legend(loc='upper right', fontsize=size_legend)\n",
    "\n",
    "    fig.savefig(f'{CWD}/models/Model_{model_num}_Training.png', bbox_inches='tight')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package='capstone',name='train_base_CNN')\n",
    "def train_base_CNN(model_num, data_loader, params, validation_loader):\n",
    "    model = make_standard_ResNet50_V2()\n",
    "    if (params['optimizer'] == 'Adam'):\n",
    "        optimizer = tf.keras.optimizers.Adam(params['learning_rate']) # Adam\n",
    "    elif (params['optimizer'] == 'SGD'):\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = params['learning_rate']) # stochastic gradient descent\n",
    "    # Binary classification task necessitates binary crossentropy loss\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "\n",
    "    train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    train_loss_metric = tf.keras.metrics.BinaryCrossentropy()\n",
    "    val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    val_loss_metric = tf.keras.metrics.BinaryCrossentropy()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x,y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x, training=True)\n",
    "            loss_value = loss_fn(y, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        train_acc_metric.update_state(y, logits)\n",
    "        train_loss_metric.update_state(y, logits)\n",
    "        return loss_value\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(x,y):\n",
    "        logits = model(x, training=False)\n",
    "        val_acc_metric.update_state(y, logits)\n",
    "        val_loss_metric.update_state(y, logits)\n",
    "\n",
    "    completed_epochs = 0 \n",
    "    ### Conditions for Early Stopping ###\n",
    "    wait_val_loss = 0\n",
    "    best_val_loss = float('inf')\n",
    "    ### Conditions for Adjusting Learning Rate ###\n",
    "    wait_val_acc = 0\n",
    "    best_val_acc = float(0)\n",
    "\n",
    "    ### Store Metrics ###\n",
    "    TRAIN_ACC = []\n",
    "    TRAIN_LOSS = []\n",
    "    VAL_ACC = []\n",
    "    VAL_LOSS = []\n",
    "\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "\n",
    "        ### Training Steps ###\n",
    "        for idx in tqdm(range(data_loader.get_train_size()//params[\"batch_size\"])):\n",
    "            x, y = data_loader.get_batch(params['batch_size'])\n",
    "            loss_value = train_step(x, y)\n",
    "        \n",
    "        train_acc = train_acc_metric.result()\n",
    "        train_loss = train_loss_metric.result()\n",
    "        train_acc_metric.reset_states()\n",
    "        train_loss_metric.reset_states()\n",
    "        print(\"Training acc over epoch: %.4f\" % (train_acc.numpy()))\n",
    "        print(\"Training loss over epoch: %.4f\" % (train_loss.numpy()))\n",
    "\n",
    "        ### Validation Steps ###\n",
    "        for idx in tqdm(range(validation_loader.get_train_size()//params['batch_size'])):\n",
    "            x_val, y_val = validation_loader.get_batch(params['batch_size'])\n",
    "            test_step(x_val, y_val)\n",
    "        \n",
    "        val_acc = val_acc_metric.result()\n",
    "        val_loss = val_loss_metric.result()\n",
    "        val_acc_metric.reset_states()\n",
    "        val_loss_metric.reset_states()\n",
    "        print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "        print(\"Validation loss: %.4f\" % (float(val_loss),))\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "        ### Store Metrics ###\n",
    "        TRAIN_ACC.append(float(train_acc))\n",
    "        TRAIN_LOSS.append(float(train_loss))\n",
    "        VAL_ACC.append(float(val_acc))\n",
    "        VAL_LOSS.append(float(val_loss))\n",
    "\n",
    "        completed_epochs += 1\n",
    "        wait_val_loss += 1\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            wait_val_loss = 0\n",
    "        if wait_val_loss >= params['patience_stop']:\n",
    "            print(\"EARLY STOP\")\n",
    "            break\n",
    "\n",
    "        wait_val_acc += 1\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            wait_val_acc = 0\n",
    "        if wait_val_acc >= params['patience_lr']:\n",
    "            if (optimizer.lr != params['min_lr']): # if the optimizer isn't at minimum LR\n",
    "                print(\"learning rate changed\")\n",
    "                old_lr = optimizer.lr\n",
    "                optimizer.lr = old_lr * params['factor'] # multiply optimizer LR by factor\n",
    "                new_lr = optimizer.lr\n",
    "                assert abs(old_lr - new_lr) < 1e-9 # ensure the rate actually did change\n",
    "                # I set the tolerance to 1e-9; that's way below the min LR.\n",
    "    \n",
    "    ### Compile Model Training Metrics ###\n",
    "    metrics = {\n",
    "        'train_acc': TRAIN_ACC,\n",
    "        'train_loss': TRAIN_LOSS,\n",
    "        'val_acc': VAL_ACC,\n",
    "        'val_loss': VAL_LOSS,\n",
    "    }\n",
    "    metrics = pd.DataFrame(metrics)\n",
    "    print(metrics)\n",
    "\n",
    "    ### Visualize Training Metrics ###\n",
    "    graph_metrics(model_num, completed_epochs, metrics)\n",
    "\n",
    "    ### Save Model ###\n",
    "    model.save(f'{CWD}/models/Model_{model_num}.keras')\n",
    "    \n",
    "    return completed_epochs, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable(package='capstone',name='train_DB_VAE')\n",
    "def train_DB_VAE(model_num, data_loader, params, validation_loader):\n",
    "    model = DB_VAE(params['latent_dim'])\n",
    "\n",
    "    if (params['optimizer'] == 'Adam'):\n",
    "        optimizer = tf.keras.optimizers.Adam(params['learning_rate']) # Adam\n",
    "    elif (params['optimizer'] == 'SGD'):\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate = params['learning_rate']) # stochastic gradient descent\n",
    "\n",
    "    ### We'll track typical binary losses as usual ###\n",
    "    train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    train_loss_metric = tf.keras.metrics.BinaryCrossentropy()\n",
    "    val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    val_loss_metric = tf.keras.metrics.BinaryCrossentropy()\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def debiasing_train_step(x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_logit, z_mean, z_logsigma, x_recon = model(x)\n",
    "            loss, class_loss = debiasing_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        train_acc_metric.update_state(y, y_logit)\n",
    "        train_loss_metric.update_state(y, y_logit)\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(x,y):\n",
    "        logits, _, _, _ = model(x, training=False)\n",
    "        val_acc_metric.update_state(y, logits)\n",
    "        val_loss_metric.update_state(y, logits)\n",
    "\n",
    "    all_imgs = data_loader.get_all_train_faces()\n",
    "\n",
    "    completed_epochs = 0 \n",
    "    ### Store Metrics ###\n",
    "    TRAIN_ACC = []\n",
    "    TRAIN_LOSS = []\n",
    "    VAL_ACC = []\n",
    "    VAL_LOSS = []\n",
    "    DB_LOSS = []\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "\n",
    "        p_lesions = get_training_sample_probabilities(all_imgs, model)\n",
    "        db_loss = np.empty(shape=[data_loader.get_train_size() // params[\"batch_size\"]])\n",
    "\n",
    "        for idx in tqdm(range(data_loader.get_train_size() // params[\"batch_size\"])):\n",
    "            # load a batch of data\n",
    "            (x, y) = data_loader.get_batch(params[\"batch_size\"], p_pos=p_lesions)\n",
    "\n",
    "            # loss optimization\n",
    "            loss = debiasing_train_step(x, y)\n",
    "            db_loss[idx] = loss\n",
    "        \n",
    "        train_acc = train_acc_metric.result()\n",
    "        train_loss = train_loss_metric.result()\n",
    "        db_loss = db_loss.mean()\n",
    "        train_acc_metric.reset_states()\n",
    "        train_loss_metric.reset_states()\n",
    "        print(\"Training acc over epoch: %.4f\" % (train_acc.numpy()))\n",
    "        print(\"Training loss over epoch: %.4f\" % (train_loss.numpy()))\n",
    "        print(\"Debiasing loss over epoch: %.4f\" % (db_loss.mean()))\n",
    "\n",
    "        for idx in tqdm(range(validation_loader.get_train_size()//params['batch_size'])):\n",
    "            x_val, y_val = validation_loader.get_batch(params['batch_size'])\n",
    "            test_step(x_val, y_val)\n",
    "        \n",
    "        val_acc = val_acc_metric.result()\n",
    "        val_loss = val_loss_metric.result()\n",
    "        val_acc_metric.reset_states()\n",
    "        val_loss_metric.reset_states()\n",
    "        print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "        print(\"Validation loss: %.4f\" % (float(val_loss),))\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "        \n",
    "        ### Store Metrics ###\n",
    "        TRAIN_ACC.append(float(train_acc))\n",
    "        TRAIN_LOSS.append(float(train_loss))\n",
    "        VAL_ACC.append(float(val_acc))\n",
    "        VAL_LOSS.append(float(val_loss))\n",
    "        DB_LOSS.append(float(db_loss))\n",
    "\n",
    "        completed_epochs += 1\n",
    "\n",
    "    ### Compile Model Training Metrics ###\n",
    "    metrics = {\n",
    "        'train_acc': TRAIN_ACC,\n",
    "        'train_loss': TRAIN_LOSS,\n",
    "        'val_acc': VAL_ACC,\n",
    "        'val_loss': VAL_LOSS,\n",
    "    }\n",
    "    metrics = pd.DataFrame(metrics)\n",
    "    print(metrics)\n",
    "\n",
    "    ### Visualize Training Metrics ###\n",
    "    graph_metrics(model_num, completed_epochs, metrics)\n",
    "\n",
    "    ### Save Model ###\n",
    "    model.save(f'{CWD}/models/Model_{model_num}.keras')\n",
    "\n",
    "    return completed_epochs, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "completed = []\n",
    "training_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Automated Training ###\n",
    "if ('models' not in os.listdir(CWD)):\n",
    "    os.mkdir(f'{CWD}/models/')\n",
    "for i in range(1,7):\n",
    "    if (i % 3 == 1):\n",
    "        loader = loader_ISIC\n",
    "        validation_loader = loader_val_ISIC\n",
    "    elif (i % 3 == 2):\n",
    "        loader = loader_ISIC_DiDI\n",
    "        validation_loader = loader_val_ISIC_DiDI\n",
    "    elif (i % 3 == 0):\n",
    "        loader = loader_ISIC_ArGI\n",
    "        validation_loader = loader_val_ISIC_ArGI\n",
    "    \n",
    "    if (i <= 3):\n",
    "        completed_epochs, metrics = train_base_CNN(i, loader, params_CNN, validation_loader)\n",
    "    elif (i <= 6):\n",
    "        completed_epochs, metrics = train_DB_VAE(i, loader, params_DB_VAE, validation_loader)\n",
    "\n",
    "    completed.append(completed_epochs)\n",
    "    training_metrics.append(metrics)\n",
    "\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Ethics\n",
    "This code makes use of the mitdeeplearning package (Amini, 2024) for the data loading function. \n",
    "The DB-VAE for Models 4-6 is inspired by the Debiasing Computer Vision Lab notebook from 6.S191.\n",
    "\n",
    "### Copyright 2024 MIT 6.S191 Introduction to Deep Learning. All Rights Reserved. \n",
    " \n",
    "Licensed under the MIT License. You may not use this file except in compliance \n",
    "with the License. Use and/or modification of this code outside of 6.S191 must \n",
    "reference: \n",
    "\n",
    "Â© MIT 6.S191: Introduction to Deep Learning \n",
    "http://introtodeeplearning.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Training Metadata ###\n",
    "\n",
    "end_time = time.localtime(time.time())\n",
    "\n",
    "f = open(f\"{CWD}/models/MODEL_INFO.txt\", \"a\")\n",
    "f.write(f\"Start Time: {start_time.tm_year}/{start_time.tm_mon}/{start_time.tm_mday}, {start_time.tm_hour}:{start_time.tm_min}:{start_time.tm_sec}\\n\")\n",
    "f.write(f\"End Time: {end_time.tm_year}/{end_time.tm_mon}/{end_time.tm_mday}, {end_time.tm_hour}:{end_time.tm_min}:{end_time.tm_sec}\")\n",
    "f.write('\\n')\n",
    "f.write(\"params_CNN = {\\n\")\n",
    "f.write(f'    optimizer = {params_CNN[\"optimizer\"]}\\n')\n",
    "f.write(f'    batch_size = {params_CNN[\"batch_size\"]}\\n')\n",
    "f.write(f'    num_epochs = {params_CNN[\"num_epochs\"]}\\n')\n",
    "f.write(f'    learning_rate = {params_CNN[\"learning_rate\"]}\\n')\n",
    "f.write(f'    min_lr = {params_CNN[\"min_lr\"]}\\n')\n",
    "f.write(f'    factor = {params_CNN[\"factor\"]}\\n')\n",
    "f.write(f'    patience_lr = {params_CNN[\"patience_lr\"]}\\n')\n",
    "f.write(f'    patience_stop = {params_CNN[\"patience_stop\"]}\\n')\n",
    "f.write('}\\n')\n",
    "f.write('\\n')\n",
    "f.write(\"params_DB_VAE = {\\n\")\n",
    "f.write(f'    optimizer = {params_DB_VAE[\"optimizer\"]}\\n')\n",
    "f.write(f'    batch_size = {params_DB_VAE[\"batch_size\"]}\\n')\n",
    "f.write(f'    num_epochs = {params_DB_VAE[\"num_epochs\"]}\\n')\n",
    "f.write(f'    learning_rate = {params_DB_VAE[\"learning_rate\"]}\\n')\n",
    "f.write(f'    latent_dim = {params_DB_VAE[\"latent_dim\"]}\\n')\n",
    "f.write('}\\n')\n",
    "f.write('\\n')\n",
    "for i in range(6):\n",
    "    f.write(f'Model {i+1} completed {completed[i]} training epochs.\\n')\n",
    "    training_metrics[i].to_excel(f'{CWD}/models/Model_{i+1}_training_metadata.xlsx')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
