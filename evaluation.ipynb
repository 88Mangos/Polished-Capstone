{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import IPython\n",
    "\n",
    "import h5py\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "import functools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/AP Research/Polished-Capstone\n"
     ]
    }
   ],
   "source": [
    "CWD = os.getcwd()\n",
    "print(CWD)\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Which models do you want? ###\n",
    "year = 2024\n",
    "month = 5\n",
    "day = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard CNN ###\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "@keras.saving.register_keras_serializable(package='capstone', name='resize_images')\n",
    "def resize_images(x):\n",
    "    return tf.image.resize(x, (64, 64))\n",
    "\n",
    "# CNN Function\n",
    "@keras.saving.register_keras_serializable(package='capstone', name='make_standard_ResNet50_V2')\n",
    "def make_standard_ResNet50_V2(n_outputs = 1):\n",
    "    \n",
    "    Resize = tf.keras.layers.Lambda(resize_images)\n",
    "    Flatten = tf.keras.layers.Flatten\n",
    "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
    "    ResNet50V2 = tf.keras.applications.ResNet50V2(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\", # Utilizing Transfer Learning, also maintains consistency\n",
    "        input_tensor=None,\n",
    "        input_shape=(64,64,3),\n",
    "        pooling=None,\n",
    "        classes=1000,\n",
    "        classifier_activation=\"softmax\",\n",
    "    )\n",
    "    ResNet50V2 = tf.keras.Model(inputs = ResNet50V2.layers[1].input, \n",
    "                                outputs = ResNet50V2.layers[-1].output)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(Resize)\n",
    "    model.add(ResNet50V2)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dense(n_outputs, activation=None))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DB-VAE ###\n",
    "\n",
    "### Define Decoder Network ###\n",
    "@keras.saving.register_keras_serializable(package='capstone', name='make_decoder_network')\n",
    "def make_decoder_network(latent_dim = 100, n_filters = 12 ):\n",
    "    \"\"\"\n",
    "    Layer Types, Functional Definition\n",
    "    \"\"\"\n",
    "    Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu')\n",
    "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
    "    Reshape = tf.keras.layers.Reshape \n",
    "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "    LeakyReLU = tf.keras.layers.LeakyReLU\n",
    "    # Decoder\n",
    "    decoder = tf.keras.Sequential([\n",
    "        Dense(units=4*4*6*n_filters),\n",
    "        Reshape(target_shape=(4,4,6*n_filters)),\n",
    "\n",
    "        Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "\n",
    "        Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "\n",
    "        Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "\n",
    "        Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return decoder\n",
    "\n",
    "### DB_VAE Helper Functions ###\n",
    "\n",
    "\n",
    "### VAE Reparameterization ###\n",
    "@keras.saving.register_keras_serializable(package='capstone', name='sampling_VAE_reparameterization')\n",
    "def sampling(z_mean, z_logsigma):\n",
    "    batch, latent_dim = z_mean.shape\n",
    "    epsilon = tf.random.normal(shape=(batch, latent_dim))\n",
    "    z = z_mean + tf.math.exp(0.5 * z_logsigma) * epsilon\n",
    "    return z\n",
    "\n",
    "### Defining the VAE loss function ###\n",
    "@keras.saving.register_keras_serializable(package='capstone', name='vae_loss_function')\n",
    "def vae_loss_function(x, x_recon, mu, logsigma, kl_weight=0.0005):\n",
    "  latent_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1.0 - logsigma, axis=1)\n",
    "  reconstruction_loss = tf.reduce_mean(tf.abs(x-x_recon), axis=(1,2,3))\n",
    "  vae_loss = kl_weight * latent_loss + reconstruction_loss\n",
    "  return vae_loss\n",
    "\n",
    "### Loss function for DB-VAE ###\n",
    "@keras.saving.register_keras_serializable(package='capstone',name='debiasing_loss_function')\n",
    "def debiasing_loss_function(x, x_pred, y, y_logit, mu, logsigma):\n",
    "  vae_loss = vae_loss_function(x, x_pred, mu, logsigma)\n",
    "  classification_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_logit)\n",
    "  malignance_indicator = tf.cast(tf.equal(y, 1), tf.float32)\n",
    "  total_loss = tf.reduce_mean(\n",
    "      classification_loss +\n",
    "      malignance_indicator * vae_loss\n",
    "  )\n",
    "  return total_loss, classification_loss\n",
    "\n",
    "### Defining and creating the DB-VAE ###\n",
    "@keras.saving.register_keras_serializable(package='capstone')\n",
    "class DB_VAE(tf.keras.Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(DB_VAE, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    # Define the number of outputs for the encoder. Recall that we have\n",
    "    # `latent_dim` latent variables, as well as a supervised output for the\n",
    "    # classification.\n",
    "    num_encoder_dims = 2*self.latent_dim + 1\n",
    "\n",
    "    self.encoder = make_standard_ResNet50_V2(num_encoder_dims)\n",
    "    self.decoder = make_decoder_network()\n",
    "\n",
    "  def encode(self, x):\n",
    "    encoder_output = self.encoder(x)\n",
    "    y_logit = tf.expand_dims(encoder_output[:, 0], -1)\n",
    "    z_mean = encoder_output[:, 1:self.latent_dim+1]\n",
    "    z_logsigma = encoder_output[:, self.latent_dim+1:]\n",
    "\n",
    "    return y_logit, z_mean, z_logsigma\n",
    "\n",
    "  def reparameterize(self, z_mean, z_logsigma):\n",
    "    z = sampling(z_mean, z_logsigma)\n",
    "    return z\n",
    "\n",
    "  def decode(self, z):\n",
    "    reconstruction = self.decoder(z)\n",
    "    return reconstruction\n",
    "\n",
    "  def call(self, x):\n",
    "    y_logit, z_mean, z_logsigma = self.encode(x)\n",
    "    z = self.reparameterize(z_mean, z_logsigma)\n",
    "    recon = self.decode(z)\n",
    "    return y_logit, z_mean, z_logsigma, recon\n",
    "\n",
    "  def predict(self, x):\n",
    "    y_logit, z_mean, z_logsigma = self.encode(x)\n",
    "    return y_logit\n",
    "  \n",
    "### DB_VAE Training Helper Functions ###\n",
    "\n",
    "# Function to return the means for an input image batch\n",
    "@keras.saving.register_keras_serializable(package='capstone',name='get_latent_mu')\n",
    "def get_latent_mu(images, dbvae, batch_size=1024, latent_dim=100):\n",
    "    N = images.shape[0]\n",
    "    mu = np.zeros((N, latent_dim))\n",
    "    for start_ind in range(0, N, batch_size):\n",
    "        end_ind = min(start_ind+batch_size, N+1)\n",
    "        batch = (images[start_ind:end_ind]).astype(np.float32)/255.\n",
    "        _, batch_mu, _ = dbvae.encode(batch)\n",
    "        mu[start_ind:end_ind] = batch_mu\n",
    "    return mu\n",
    "\n",
    "@keras.saving.register_keras_serializable(package='capstone',name='get_training_sample_probabilities')\n",
    "def get_training_sample_probabilities(images, dbvae, bins=10, smoothing_fac=0.001, latent_dim=100):\n",
    "    print(\"Recomputing the sampling probabilities\")\n",
    "    mu = get_latent_mu(images, dbvae)\n",
    "    training_sample_p = np.zeros(mu.shape[0])\n",
    "    for i in range(latent_dim):\n",
    "        latent_distribution = mu[:,i]\n",
    "        hist_density, bin_edges =  np.histogram(latent_distribution, density=True, bins=bins)\n",
    "        bin_edges[0] = -float('inf')\n",
    "        bin_edges[-1] = float('inf')\n",
    "        bin_idx = np.digitize(latent_distribution, bin_edges)\n",
    "        hist_smoothed_density = hist_density + smoothing_fac\n",
    "        hist_smoothed_density = hist_smoothed_density / np.sum(hist_smoothed_density)\n",
    "        p = 1.0/(hist_smoothed_density[bin_idx-1])\n",
    "        p = p / np.sum(p)\n",
    "        training_sample_p = np.maximum(p, training_sample_p)\n",
    "    training_sample_p /= np.sum(training_sample_p)\n",
    "\n",
    "    return training_sample_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 7):\n",
    "    models.append(keras.models.load_model(f'{CWD}/models/{year}-{month}-{day}_model_{i}.keras', safe_mode=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = keras.models.load_model(f'{CWD}/models/{year}-{month}-{day}_model_{4}.keras', safe_mode=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
