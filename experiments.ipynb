{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import IPython\n",
    "\n",
    "import h5py\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "import functools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Parameters\n",
    "COMET_API_KEY = 't6Sqtes2FLSBv39Opo3q8TmVk'\n",
    "matplotlib.rcParams['font.family'] = \"Times New Roman\"\n",
    "\n",
    "CWD = os.getcwd()\n",
    "print(CWD)\n",
    "\n",
    "current_time = time.gmtime(time.time())\n",
    "print(current_time)\n",
    "\n",
    "### Create a Comet experiment to track our training run ###\n",
    "def create_experiment(project_name, params):\n",
    "    # end any prior experiments\n",
    "    if 'experiment' in locals():\n",
    "        experiment.end()\n",
    "\n",
    "    # initiate the comet experiment for tracking\n",
    "    experiment = comet_ml.Experiment(\n",
    "    api_key=COMET_API_KEY,\n",
    "    project_name=project_name)\n",
    "    # log our hyperparameters, defined above, to the experiment\n",
    "    for param, value in params.items():\n",
    "        experiment.log_parameter(param, value)\n",
    "    experiment.flush()\n",
    "\n",
    "    return experiment\n",
    "\n",
    "### Ensure training on GPU ###\n",
    "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
    "assert COMET_API_KEY != \"\", \"Please insert your Comet API Key\"\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_to_training_data):\n",
    "    with h5py.File(path_to_training_data) as f:\n",
    "        # Print the keys (names) of all groups and datasets in the file\n",
    "        print(\"Keys:\", list(f.keys()))\n",
    "\n",
    "        # Iterate through each key and print more detailed information\n",
    "        for key in f.keys():\n",
    "            if isinstance(f[key], h5py.Dataset):\n",
    "                print(f\"Dataset: {key}\")\n",
    "                print(\"  Shape:\", f[key].shape)\n",
    "                print(\"  Data type:\", f[key].dtype)\n",
    "                \n",
    "    ### Instantiate Loader Function ###\n",
    "    return mdl.lab2.TrainingDatasetLoader(path_to_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(path_to_training_data, loader):\n",
    "    ### Visualize our data ###\n",
    "    number_of_training_examples = loader.get_train_size()\n",
    "    print(number_of_training_examples)\n",
    "    (images, labels) = loader.get_batch(100)\n",
    "    malignant_images = images[np.where(labels==1)[0]]\n",
    "    benign_images = images[np.where(labels==0)[0]]\n",
    "\n",
    "    idx_malignant = 23\n",
    "    idx_benign = 9\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(malignant_images[idx_malignant])\n",
    "    plt.title(\"Malignant\"); plt.grid(False)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(benign_images[idx_benign])\n",
    "    plt.title(\"Benign\"); plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instantiate Loaders ###\n",
    "loader_ISIC = load_dataset(f'{CWD}/datasets/train_ISIC.h5')\n",
    "loader_ISIC_DiDI = load_dataset(f'{CWD}/datasets/train_ISIC_DiDI.h5')\n",
    "loader_ISIC_ArGI = load_dataset(f'{CWD}/datasets/train_ISIC_ArGI.h5')\n",
    "loader_test = load_dataset(f'{CWD}/datasets/test_ISIC_DiDI.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize Training Datasets ###\n",
    "visualize_dataset(f'{CWD}/datasets/train_ISIC.h5', loader_ISIC)\n",
    "visualize_dataset(f'{CWD}/datasets/train_ISIC_DiDI.h5', loader_ISIC_DiDI)\n",
    "visualize_dataset(f'{CWD}/datasets/train_ISIC_ArGI.h5', loader_ISIC_ArGI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard CNN ###\n",
    "\n",
    "# Helper Functions\n",
    "def resize_images(x):\n",
    "    return tf.image.resize(x, (64, 64))\n",
    "\n",
    "# CNN Function\n",
    "def make_standard_ResNet50_V2(n_outputs = 1):\n",
    "    \n",
    "    Resize = tf.keras.layers.Lambda(resize_images)\n",
    "    Flatten = tf.keras.layers.Flatten\n",
    "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
    "    ResNet50V2 = tf.keras.applications.ResNet50V2(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\", # Utilizing Transfer Learning, also maintains consistency\n",
    "        input_tensor=None,\n",
    "        input_shape=(64,64,3),\n",
    "        pooling=None,\n",
    "        classes=1000,\n",
    "        classifier_activation=\"softmax\",\n",
    "    )\n",
    "    ResNet50V2 = tf.keras.Model(inputs = ResNet50V2.layers[1].input, \n",
    "                                outputs = ResNet50V2.layers[-1].output)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(Resize)\n",
    "    model.add(ResNet50V2)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dense(n_outputs, activation=None))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DB-VAE ###\n",
    "\n",
    "### Define Decoder Network ###\n",
    "latent_dim = 100 # number of latent variables\n",
    "n_filters = 12 \n",
    "def make_decoder_network():\n",
    "    \"\"\"\n",
    "    Layer Types, Functional Definition\n",
    "    \"\"\"\n",
    "    Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu')\n",
    "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
    "    Reshape = tf.keras.layers.Reshape \n",
    "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "    LeakyReLU = tf.keras.layers.LeakyReLU\n",
    "    # Decoder\n",
    "    decoder = tf.keras.Sequential([\n",
    "        Dense(units=4*4*6*n_filters),\n",
    "        Reshape(target_shape=(4,4,6*n_filters)),\n",
    "\n",
    "        Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "\n",
    "        Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "\n",
    "        Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same'),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "\n",
    "        Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return decoder\n",
    "\n",
    "### DB_VAE Helper Functions ###\n",
    "\n",
    "### VAE Reparameterization ###\n",
    "def sampling(z_mean, z_logsigma):\n",
    "    batch, latent_dim = z_mean.shape\n",
    "    epsilon = tf.random.normal(shape=(batch, latent_dim))\n",
    "    z = z_mean + tf.math.exp(0.5 * z_logsigma) * epsilon\n",
    "    return z\n",
    "\n",
    "### Defining the VAE loss function ###\n",
    "def vae_loss_function(x, x_recon, mu, logsigma, kl_weight=0.0005):\n",
    "  latent_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1.0 - logsigma, axis=1)\n",
    "  reconstruction_loss = tf.reduce_mean(tf.abs(x-x_recon), axis=(1,2,3))\n",
    "  vae_loss = kl_weight * latent_loss + reconstruction_loss\n",
    "  return vae_loss\n",
    "\n",
    "### Loss function for DB-VAE ###\n",
    "def debiasing_loss_function(x, x_pred, y, y_logit, mu, logsigma):\n",
    "  vae_loss = vae_loss_function(x, x_pred, mu, logsigma)\n",
    "  classification_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_logit)\n",
    "  malignance_indicator = tf.cast(tf.equal(y, 1), tf.float32)\n",
    "  total_loss = tf.reduce_mean(\n",
    "      classification_loss +\n",
    "      malignance_indicator * vae_loss\n",
    "  )\n",
    "  return total_loss, classification_loss\n",
    "\n",
    "### Defining and creating the DB-VAE ###\n",
    "\n",
    "class DB_VAE(tf.keras.Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(DB_VAE, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "\n",
    "    # Define the number of outputs for the encoder. Recall that we have\n",
    "    # `latent_dim` latent variables, as well as a supervised output for the\n",
    "    # classification.\n",
    "    num_encoder_dims = 2*self.latent_dim + 1\n",
    "\n",
    "    self.encoder = make_standard_ResNet50_V2(num_encoder_dims)\n",
    "    self.decoder = make_decoder_network()\n",
    "\n",
    "  def encode(self, x):\n",
    "    encoder_output = self.encoder(x)\n",
    "    y_logit = tf.expand_dims(encoder_output[:, 0], -1)\n",
    "    z_mean = encoder_output[:, 1:self.latent_dim+1]\n",
    "    z_logsigma = encoder_output[:, self.latent_dim+1:]\n",
    "\n",
    "    return y_logit, z_mean, z_logsigma\n",
    "\n",
    "  def reparameterize(self, z_mean, z_logsigma):\n",
    "    z = sampling(z_mean, z_logsigma)\n",
    "    return z\n",
    "\n",
    "  def decode(self, z):\n",
    "    reconstruction = self.decoder(z)\n",
    "    return reconstruction\n",
    "\n",
    "  def call(self, x):\n",
    "    y_logit, z_mean, z_logsigma = self.encode(x)\n",
    "    z = self.reparameterize(z_mean, z_logsigma)\n",
    "    recon = self.decode(z)\n",
    "    return y_logit, z_mean, z_logsigma, recon\n",
    "\n",
    "  def predict(self, x):\n",
    "    y_logit, z_mean, z_logsigma = self.encode(x)\n",
    "    return y_logit\n",
    "  \n",
    "### DB_VAE Training Helper Functions ###\n",
    "\n",
    "# Function to return the means for an input image batch\n",
    "def get_latent_mu(images, dbvae, batch_size=1024):\n",
    "    N = images.shape[0]\n",
    "    mu = np.zeros((N, latent_dim))\n",
    "    for start_ind in range(0, N, batch_size):\n",
    "        end_ind = min(start_ind+batch_size, N+1)\n",
    "        batch = (images[start_ind:end_ind]).astype(np.float32)/255.\n",
    "        _, batch_mu, _ = dbvae.encode(batch)\n",
    "        mu[start_ind:end_ind] = batch_mu\n",
    "    return mu\n",
    "\n",
    "def get_training_sample_probabilities(images, dbvae, bins=10, smoothing_fac=0.001):\n",
    "    print(\"Recomputing the sampling probabilities\")\n",
    "    mu = get_latent_mu(images, dbvae)\n",
    "    training_sample_p = np.zeros(mu.shape[0])\n",
    "    for i in range(latent_dim):\n",
    "        latent_distribution = mu[:,i]\n",
    "        hist_density, bin_edges =  np.histogram(latent_distribution, density=True, bins=bins)\n",
    "        bin_edges[0] = -float('inf')\n",
    "        bin_edges[-1] = float('inf')\n",
    "        bin_idx = np.digitize(latent_distribution, bin_edges)\n",
    "        hist_smoothed_density = hist_density + smoothing_fac\n",
    "        hist_smoothed_density = hist_smoothed_density / np.sum(hist_smoothed_density)\n",
    "        p = 1.0/(hist_smoothed_density[bin_idx-1])\n",
    "        p = p / np.sum(p)\n",
    "        training_sample_p = np.maximum(p, training_sample_p)\n",
    "    training_sample_p /= np.sum(training_sample_p)\n",
    "\n",
    "    return training_sample_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set Training Hyperparamters ###\n",
    "\n",
    "### Hyperparameters for CNN Training ###\n",
    "params_CNN = dict( \n",
    "  batch_size = 32,\n",
    "  num_epochs = 50,\n",
    "  learning_rate = 5e-4,\n",
    ")\n",
    "\n",
    "### Hyperparameters for DB-VAE Training ###\n",
    "params_DB_VAE = dict(\n",
    "    batch_size = 32,\n",
    "    num_epochs = 50, \n",
    "    learning_rate = 5e-4,\n",
    "    latent_dim = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_base_CNN(model_num, data_loader, params):\n",
    "\n",
    "    ### Instantiate new model ###\n",
    "    model = make_standard_ResNet50_V2()\n",
    "\n",
    "    ### Train the standard CNN ###\n",
    "    experiment = create_experiment(f\"{current_time.tm_year}-{current_time.tm_mon}-{current_time.tm_mday}_Model_{model_num}\", params)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(params[\"learning_rate\"]) # define our optimizer\n",
    "    loss_history = mdl.util.LossHistory(smoothing_factor=0.99) # to record loss evolution\n",
    "    plotter = mdl.util.PeriodicPlotter(sec=2, scale='semilogy')\n",
    "    if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "    @tf.function\n",
    "    def standard_train_step(x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # feed the images into the model\n",
    "            logits = model(x)\n",
    "            # Compute the loss\n",
    "            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    # The training loop!\n",
    "    step = 0\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        for idx in tqdm(range(data_loader.get_train_size()//params[\"batch_size\"])):\n",
    "            # Grab a batch of training data and propagate through the network\n",
    "            x, y = data_loader.get_batch(params[\"batch_size\"])\n",
    "\n",
    "            loss = standard_train_step(x, y)\n",
    "\n",
    "            # Record the loss and plot the evolution of the loss as a function of training\n",
    "            loss_history.append(loss.numpy().mean())\n",
    "            plotter.plot(loss_history.get())\n",
    "\n",
    "            experiment.log_metric(\"loss\", loss.numpy().mean(), step=step)\n",
    "            step += 1\n",
    "\n",
    "    ### Obtain Loss Values Over Epoch ###\n",
    "    steps = len(loss_history.get())\n",
    "    print(steps)\n",
    "\n",
    "    epochs = np.uint8(params['num_epochs'])\n",
    "    batches_per_epoch = np.uint8(steps/epochs)\n",
    "\n",
    "    loss_hist = np.zeros((steps,1))\n",
    "    for i in range(steps):\n",
    "        loss_hist[i] = loss_history.get()[i]\n",
    "    loss_hist = loss_hist.reshape(epochs, batches_per_epoch)\n",
    "\n",
    "    row_means = np.zeros((epochs))\n",
    "    for i in range(epochs):\n",
    "        row_means[i] = loss_hist.sum(axis=1)[i]\n",
    "\n",
    "    row_means = row_means/batches_per_epoch\n",
    "    for mean in row_means:\n",
    "        print(mean)\n",
    "\n",
    "    experiment.end()\n",
    "    return model, row_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_DB_VAE(model_num, data_loader, params):\n",
    "    \n",
    "    ### Instantiate new model ###\n",
    "    model = DB_VAE(params[\"latent_dim\"])\n",
    "    \n",
    "    experiment = create_experiment(f\"{current_time.tm_year}-{current_time.tm_mon}-{current_time.tm_mday}_Model_{model_num}\", params)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(params[\"learning_rate\"])\n",
    "\n",
    "    @tf.function\n",
    "    def debiasing_train_step(x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_logit, z_mean, z_logsigma, x_recon = model(x)\n",
    "            loss, class_loss = debiasing_loss_function(x, x_recon, y, y_logit, z_mean, z_logsigma)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "    all_imgs = data_loader.get_all_train_faces()\n",
    "\n",
    "    if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "    loss_history_2 = mdl.util.LossHistory(smoothing_factor=0.99) # to record loss evolution\n",
    "    # The training loop -- outer loop iterates over the number of epochs\n",
    "    step = 0\n",
    "    for i in range(params[\"num_epochs\"]):\n",
    "\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        print(\"Starting epoch {}/{}\".format(i+1, params[\"num_epochs\"]))\n",
    "        p_lesions = get_training_sample_probabilities(all_imgs, model)\n",
    "\n",
    "        for j in tqdm(range(data_loader.get_train_size() // params[\"batch_size\"])):\n",
    "            # load a batch of data\n",
    "            (x, y) = data_loader.get_batch(params[\"batch_size\"], p_pos=p_lesions)\n",
    "\n",
    "            # loss optimization\n",
    "            loss = debiasing_train_step(x, y)\n",
    "            experiment.log_metric(\"loss\", loss.numpy().mean(), step=step, epoch=i+1)\n",
    "            loss_history_2.append(loss.numpy().mean())\n",
    "            # plot the progress every 200 steps\n",
    "            if j % 500 == 0:\n",
    "                mdl.util.plot_sample(x, y, model)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    ### Obtain Loss Values Over Epoch ###\n",
    "    steps = len(loss_history_2.get())\n",
    "    print(steps)\n",
    "\n",
    "    epochs = np.uint8(params['num_epochs'])\n",
    "    batches_per_epoch = np.uint8(steps/epochs)\n",
    "\n",
    "    loss_hist = np.zeros((steps,1))\n",
    "    for i in range(steps):\n",
    "        loss_hist[i] = loss_history_2.get()[i]\n",
    "    loss_hist = loss_hist.reshape(epochs, batches_per_epoch)\n",
    "\n",
    "    row_means = np.zeros((epochs))\n",
    "    for i in range(epochs):\n",
    "        row_means[i] = loss_hist.sum(axis=1)[i]\n",
    "\n",
    "    row_means = row_means/batches_per_epoch\n",
    "    for mean in row_means:\n",
    "        print(mean)\n",
    "\n",
    "    experiment.end()\n",
    "\n",
    "    return model, row_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_loss(model_num, row_means, epochs):\n",
    "    epochs_range = np.arange(1,epochs+1)\n",
    "    plt.figure(figsize=(8,7))\n",
    "    size_axis_titles = 16\n",
    "    size_title = 18\n",
    "    size_legend = 14\n",
    "    plt.xlabel(\"Epoch\", fontsize=size_axis_titles)\n",
    "    plt.ylabel(\"Loss\", fontsize=size_axis_titles)\n",
    "    plt.axis([1, 50, 0, 2.5])\n",
    "    plt.plot(epochs_range, row_means, label='Training')\n",
    "    plt.legend(loc='upper right', fontsize=size_legend)\n",
    "    plt.title(f'Training Loss for Model {model_num}', fontsize=size_title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    ### Evaluation of model on test dataset (n = 526) ###\n",
    "    n=30 # allows for t-testing\n",
    "    accuracies = np.zeros((n,1))\n",
    "    for i in range(n):\n",
    "        (batch_x, batch_y) = loader_test.get_batch(256)\n",
    "        y_pred_standard = tf.round(tf.nn.sigmoid(model.predict(batch_x)))\n",
    "        acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
    "        accuracies[i] = acc_standard.numpy()\n",
    "    \n",
    "    print(accuracies.mean())\n",
    "    print(accuracies.std())\n",
    "    \n",
    "    return accuracies.mean(), accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.empty([6, 2]) # for each model, store mean accuracy and stdev of accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mitdeeplearning.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1, row_means_1 = experiment_base_CNN(1, loader_ISIC, params_CNN)\n",
    "graph_loss(1, row_means_1, params_CNN[\"num_epochs\"])\n",
    "\n",
    "results[1-1, 0], results[1-1, 1] = evaluate(model_1)\n",
    "\n",
    "model_1.save(f\"{CWD}/models/{current_time.tm_year}-{current_time.tm_mon}-{current_time.tm_mday}_model_1.keras\")\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2, row_means_2 = experiment_base_CNN(2, loader_ISIC_DiDI, params_CNN)\n",
    "graph_loss(2, row_means_2, params_CNN[\"num_epochs\"])\n",
    "\n",
    "results[2-1, 0], results[2-1, 1] = evaluate(model_2)\n",
    "\n",
    "model_2.save(f\"{CWD}/models/{current_time.tm_year}-{current_time.tm_mon}-{current_time.tm_mday}_model_2.keras\")\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3, row_means_3 = experiment_base_CNN(3, loader_ISIC_ArGI, params_CNN)\n",
    "graph_loss(3, row_means_3, params_CNN[\"num_epochs\"])\n",
    "\n",
    "results[3-1, 0], results[3-1, 1] = evaluate(model_3)\n",
    "\n",
    "model_3.save(f\"{CWD}/models/{current_time.tm_year}-{current_time.tm_mon}-{current_time.tm_mday}_model_3.keras\")\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4, row_means_4 = experiment_DB_VAE(4, loader_ISIC, params_DB_VAE)\n",
    "graph_loss(4, row_means_4, params_DB_VAE[\"num_epochs\"])\n",
    "\n",
    "results[4-1, 0], results[4-1, 1] = evaluate(model_4)\n",
    "\n",
    "model_4.save(f\"{CWD}/models/{current_time.tm_year}-{current_time.tm_mon}-{current_time.tm_mday}_model_4.keras\")\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5, row_means_5 = experiment_DB_VAE(5, loader_ISIC_DiDI, params_DB_VAE)\n",
    "graph_loss(5, row_means_5, params_DB_VAE[\"num_epochs\"])\n",
    "\n",
    "results[5-1, 0], results[5-1, 1] = evaluate(model_5)\n",
    "\n",
    "model_5.save(f\"{CWD}/models/{current_time.tm_year}-{current_time.tm_mon}-{current_time.tm_mday}_model_5.keras\")\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6, row_means_6 = experiment_DB_VAE(6, loader_ISIC_ArGI, params_DB_VAE)\n",
    "graph_loss(6, row_means_6, params_DB_VAE[\"num_epochs\"])\n",
    "\n",
    "results[6-1, 0], results[6-1, 1] = evaluate(model_6)\n",
    "\n",
    "model_6.save(f\"{CWD}/models/{current_time.tm_year}-{current_time.tm_mon}-{current_time.tm_mday}_model_6.keras\")\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Ethics\n",
    "This code makes use of the mitdeeplearning package (Amini, 2024) for the data loading function. \n",
    "The DB-VAE for Models 4-6 is inspired by the Debiasing Computer Vision Lab notebook from 6.S191.\n",
    "\n",
    "### Copyright 2024 MIT 6.S191 Introduction to Deep Learning. All Rights Reserved. \n",
    " \n",
    "Licensed under the MIT License. You may not use this file except in compliance \n",
    "with the License. Use and/or modification of this code outside of 6.S191 must \n",
    "reference: \n",
    "\n",
    "© MIT 6.S191: Introduction to Deep Learning \n",
    "http://introtodeeplearning.com "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
